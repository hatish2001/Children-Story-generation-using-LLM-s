## Abstract
  - This report investigates the intersection of text and
  image generation through the evaluation of two
  OpenAI models for the purpose of generating
  childrenâ€™s fairy tales and corresponding images. The
  generated text underwent human evaluation and a
  readability prediction using a neural network model.
  The study aims to compare the performance of two
  OpenAI models through human assessment and
  linguistic analysis.

## Introduction
- By leveraging two distinct OpenAI models to create
  narratives and developing corresponding images, we
  aim to not only showcase the capabilities of these
  models but also assess their performance through
  human evaluation and linguistic analysis tools. The
  following pages dive into the methodology, data,
  evaluation considerations, models used, results, and
  discussions of this intersection between evaluating
  AI through storytelling.

## Data
- The dataset, comprising 68 children's fairy tales
  sourced from Kaggle and Project Gutenberg,
  underwent rigorous cleaning. Two OpenAI models,
  text-davinci-003 and babbage, were selected for text
  generation based on their natural language
  processing proficiency. Preprocessing involved story
  segmentation into 'chunks,' randomly selecting 1,500
  chunks for diversity. Image generation employed the
  AutoPipelineForText2Image from Diffusers,
  pretrained on stable-diffusion-xl-base-1.0.
  LangChain facilitated creative story generation,
  summarization with Transformers provided concise prompts, and the text2image pipeline translated
  prompts into visually appealing images.

## Evaluation 
### Human Centric Evaluation
- To assess the quality of generated stories, a
  comprehensive human survey was developed.
  Participants were presented with three sets of stories
  for ranking: one generated by the babbage model,
  one by the text-davinci-003 model, and one crafted
  by a human. In total, survey takers evaluated and
  ranked nine stories, ordering them from best to
  worst. The evaluation criteria encompassed
  grammaticality, kid-friendliness, and narrative flow.
  This human-centric approach provided valuable
  insights into the perceived quality of stories,
  considering aspects crucial for children's
  engagement.

## Readability Score Evaluation
- A neural network model designed to predict the
  difficulty of a text through the analysis of words,
  sentence length, and semantics was employed for the
  readability score evaluation. Adapted from a Kaggle
  GitHub repository, the readability score ranged from
  -1 (most complex/unreadable) to 1 (most readable).
  However, before inputting the generated text into the
  readability model, a series of preprocessing steps
  were undertaken. The custom preprocessing
  functions collectively transformed raw text data into
  a machine-learning-friendly format.
