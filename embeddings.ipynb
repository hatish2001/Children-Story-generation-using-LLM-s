{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hatish2001/Children-Story-generation-using-LLM-s/blob/main/embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDsn_Motja8Q"
      },
      "source": [
        "# Cross-Language Word Embeddings\n",
        "\n",
        "In class, we discussed how we can reduce the dimensionality of word representations from their original vector space to an embedding space on the order of a few hundred dimensions. Different modeling choices for word embeddings may be ultimately evaluated by the effectiveness of classification or retrieval models.\n",
        "\n",
        "In this assignment, however, we will consider another common method of evaluating word embeddings: by judging the usefulness of pairwise distances between words in the embedding space.\n",
        "\n",
        "Follow along with the examples in this notebook, and implement the sections of code flagged with **TODO**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKm5cPMQ2xHU"
      },
      "source": [
        "# The following packages are installed by default on Colab\n",
        "# If you're running on another platform, you may need to install them with\n",
        "# pip, conda, brew, or other package managers.\n",
        "import gensim\n",
        "import numpy as np\n",
        "from gensim.test.utils import datapath, get_tmpfile\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models.word2vec import LineSentence"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfKjYFDklB4c"
      },
      "source": [
        "We'll start by downloading a plain-text version of the plays of William Shakespeare."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dw3bvl1yf5FB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "024a38ca-341b-4ceb-e6f6-016b10caeb62"
      },
      "source": [
        "!wget http://www.ccs.neu.edu/home/dasmith/courses/cs6200/shakespeare_plays.txt\n",
        "lines = [s.split() for s in open('shakespeare_plays.txt')]"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-06 21:18:26--  http://www.ccs.neu.edu/home/dasmith/courses/cs6200/shakespeare_plays.txt\n",
            "Resolving www.ccs.neu.edu (www.ccs.neu.edu)... 52.70.229.197\n",
            "Connecting to www.ccs.neu.edu (www.ccs.neu.edu)|52.70.229.197|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://www.khoury.northeastern.edu/home/dasmith/courses/cs6200/shakespeare_plays.txt [following]\n",
            "--2024-12-06 21:18:26--  https://www.khoury.northeastern.edu/home/dasmith/courses/cs6200/shakespeare_plays.txt\n",
            "Resolving www.khoury.northeastern.edu (www.khoury.northeastern.edu)... 52.70.229.197\n",
            "Connecting to www.khoury.northeastern.edu (www.khoury.northeastern.edu)|52.70.229.197|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4746840 (4.5M) [text/plain]\n",
            "Saving to: ‘shakespeare_plays.txt’\n",
            "\n",
            "shakespeare_plays.t 100%[===================>]   4.53M  10.5MB/s    in 0.4s    \n",
            "\n",
            "2024-12-06 21:18:27 (10.5 MB/s) - ‘shakespeare_plays.txt’ saved [4746840/4746840]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cZ52pEflKKM"
      },
      "source": [
        "Then, we'll estimate a simple word2vec model on the Shakespeare texts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXT5BNPs_zjM"
      },
      "source": [
        "model = Word2Vec(lines)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fzt3lG1-lw33"
      },
      "source": [
        "Even with such a small training set size, you can perform some standard analogy tasks we discussed in class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4ruAqhKC3-R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b2ad2d4-9afa-42d4-fa63-85f25a838864"
      },
      "source": [
        "model.wv.most_similar(positive=['king','woman'], negative=['man'])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('queen', 0.7983852028846741),\n",
              " ('prince', 0.7517792582511902),\n",
              " ('york', 0.7444249391555786),\n",
              " ('warwick', 0.7233059406280518),\n",
              " ('duke', 0.7192990779876709),\n",
              " ('clarence', 0.7055583000183105),\n",
              " ('princess', 0.6948035359382629),\n",
              " ('son', 0.6894858479499817),\n",
              " ('cardinal', 0.6798964142799377),\n",
              " ('percy', 0.6755530834197998)]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJL45y5emjA9"
      },
      "source": [
        "In other words, we want a vector close to `king` and `woman` but subtracting the dimensions that are important to `man`, i.e., `queen`. Other words are mostly noble titles important in Shakespeare's \"history\" plays.\n",
        "\n",
        "For the rest of this assignment, we will focus on finding words with similar embeddings, both within and across languages. For example, what words are similar to the name of the title character of *Othello*?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EZGroU0KPyj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcc35dd9-e3a1-4bca-f779-8b2bd1630807"
      },
      "source": [
        "model.wv.most_similar(positive=['othello'])\n",
        "#model.wv.most_similar(positive=['brutus'])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('desdemona', 0.9638245701789856),\n",
              " ('cleopatra', 0.9333956241607666),\n",
              " ('iago', 0.9238488078117371),\n",
              " ('mrs', 0.9182373881340027),\n",
              " ('ham', 0.9097301959991455),\n",
              " ('countess', 0.905004620552063),\n",
              " ('cassio', 0.9033157825469971),\n",
              " ('imogen', 0.9031842350959778),\n",
              " ('emilia', 0.8990395069122314),\n",
              " ('jul', 0.8906999826431274)]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UM2BT_7zZle3"
      },
      "source": [
        "If you know the play, you might see some familiar names.\n",
        "\n",
        "This search uses cosine similarity. In the default API, you should see the same similarity between the words `othello` and `desdemona` as in the search results above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-e32-u4zYFda",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff9ff78a-a41a-4b74-d1b3-6df29bb66d8f"
      },
      "source": [
        "model.wv.similarity('othello', 'desdemona')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9638246"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c49DwfAmZ6PU"
      },
      "source": [
        "**TODO**: Your **first task**, therefore, is to implement your own cosine similarity function so that you can reuse it outside of the context of the gensim model object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEj2PqpuZ5xs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "741c867f-1765-4558-9446-9a8a9fc6ffa0"
      },
      "source": [
        "## TODO: Implement cosim\n",
        "def cosim(v1, v2):\n",
        "  ## return cosine similarity between v1 and v2\n",
        "    # Compute the dot product of v1 and v2\n",
        "    dot_product = np.dot(v1, v2)\n",
        "    # Compute the norms of v1 and v2\n",
        "    norm_v1 = np.linalg.norm(v1)\n",
        "    norm_v2 = np.linalg.norm(v2)\n",
        "    # Compute the cosine similarity\n",
        "    return dot_product / (norm_v1 * norm_v2)\n",
        "\n",
        "## This should give a result similar to model.wv.similarity:\n",
        "cosim(model.wv['othello'], model.wv['desdemona'])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.96382475"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TbDqBIHbHfB"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "We could collect a lot of human judgments about how similar pairs of words, or pairs of Shakespearean characters, are. Then we could compare different word-embedding models by their ability to replicate these human judgments.\n",
        "\n",
        "If we extend our ambition to multiple languages, however, we can use a word translation task to evaluate word embeddings.\n",
        "\n",
        "We will use a subset of [Facebook AI's FastText cross-language embeddings](https://fasttext.cc/docs/en/aligned-vectors.html) for several languages. Your task will be to compare English both to French, and to *one more language* from the following set: Arabic, German, Portuguese, Russian, Spanish, Vietnamese, and Chinese."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FC_FXRnfq1BO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27020348-0c9b-47a6-c790-763ffbfeb191"
      },
      "source": [
        "!wget http://www.ccs.neu.edu/home/dasmith/courses/cs6200/30k.en.vec\n",
        "!wget http://www.ccs.neu.edu/home/dasmith/courses/cs6200/30k.fr.vec\n",
        "\n",
        "# TODO: uncomment at least one of these to work with another language\n",
        "# !wget http://www.ccs.neu.edu/home/dasmith/courses/cs6200/30k.ar.vec\n",
        "# !wget http://www.ccs.neu.edu/home/dasmith/courses/cs6200/30k.de.vec\n",
        "# !wget http://www.ccs.neu.edu/home/dasmith/courses/cs6200/30k.pt.vec\n",
        "# !wget http://www.ccs.neu.edu/home/dasmith/courses/cs6200/30k.ru.vec\n",
        "!wget http://www.ccs.neu.edu/home/dasmith/courses/cs6200/30k.es.vec\n",
        "# !wget http://www.ccs.neu.edu/home/dasmith/courses/cs6200/30k.vi.vec\n",
        "# !wget http://www.ccs.neu.edu/home/dasmith/courses/cs6200/30k.zh.vec"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-06 21:30:18--  http://www.ccs.neu.edu/home/dasmith/courses/cs6200/30k.en.vec\n",
            "Resolving www.ccs.neu.edu (www.ccs.neu.edu)... 52.70.229.197\n",
            "Connecting to www.ccs.neu.edu (www.ccs.neu.edu)|52.70.229.197|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://www.khoury.northeastern.edu/home/dasmith/courses/cs6200/30k.en.vec [following]\n",
            "--2024-12-06 21:30:18--  https://www.khoury.northeastern.edu/home/dasmith/courses/cs6200/30k.en.vec\n",
            "Resolving www.khoury.northeastern.edu (www.khoury.northeastern.edu)... 52.70.229.197\n",
            "Connecting to www.khoury.northeastern.edu (www.khoury.northeastern.edu)|52.70.229.197|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 67681172 (65M)\n",
            "Saving to: ‘30k.en.vec.1’\n",
            "\n",
            "30k.en.vec.1        100%[===================>]  64.54M  39.6MB/s    in 1.6s    \n",
            "\n",
            "2024-12-06 21:30:20 (39.6 MB/s) - ‘30k.en.vec.1’ saved [67681172/67681172]\n",
            "\n",
            "--2024-12-06 21:30:20--  http://www.ccs.neu.edu/home/dasmith/courses/cs6200/30k.fr.vec\n",
            "Resolving www.ccs.neu.edu (www.ccs.neu.edu)... 52.70.229.197\n",
            "Connecting to www.ccs.neu.edu (www.ccs.neu.edu)|52.70.229.197|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://www.khoury.northeastern.edu/home/dasmith/courses/cs6200/30k.fr.vec [following]\n",
            "--2024-12-06 21:30:20--  https://www.khoury.northeastern.edu/home/dasmith/courses/cs6200/30k.fr.vec\n",
            "Resolving www.khoury.northeastern.edu (www.khoury.northeastern.edu)... 52.70.229.197\n",
            "Connecting to www.khoury.northeastern.edu (www.khoury.northeastern.edu)|52.70.229.197|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 67802327 (65M)\n",
            "Saving to: ‘30k.fr.vec.1’\n",
            "\n",
            "30k.fr.vec.1        100%[===================>]  64.66M  40.6MB/s    in 1.6s    \n",
            "\n",
            "2024-12-06 21:30:22 (40.6 MB/s) - ‘30k.fr.vec.1’ saved [67802327/67802327]\n",
            "\n",
            "--2024-12-06 21:30:22--  http://www.ccs.neu.edu/home/dasmith/courses/cs6200/30k.es.vec\n",
            "Resolving www.ccs.neu.edu (www.ccs.neu.edu)... 52.70.229.197\n",
            "Connecting to www.ccs.neu.edu (www.ccs.neu.edu)|52.70.229.197|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://www.khoury.northeastern.edu/home/dasmith/courses/cs6200/30k.es.vec [following]\n",
            "--2024-12-06 21:30:23--  https://www.khoury.northeastern.edu/home/dasmith/courses/cs6200/30k.es.vec\n",
            "Resolving www.khoury.northeastern.edu (www.khoury.northeastern.edu)... 52.70.229.197\n",
            "Connecting to www.khoury.northeastern.edu (www.khoury.northeastern.edu)|52.70.229.197|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 67762853 (65M) [application/ecmascript]\n",
            "Saving to: ‘30k.es.vec’\n",
            "\n",
            "30k.es.vec          100%[===================>]  64.62M  40.9MB/s    in 1.6s    \n",
            "\n",
            "2024-12-06 21:30:25 (40.9 MB/s) - ‘30k.es.vec’ saved [67762853/67762853]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmuIvGpNrJPe"
      },
      "source": [
        "We'll start by loading the word vectors from their textual file format to a dictionary mapping words to numpy arrays."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbWORXkP2Vvn"
      },
      "source": [
        "def vecref(s):\n",
        "  (word, srec) = s.split(' ', 1)\n",
        "  return (word, np.fromstring(srec, sep=' '))\n",
        "\n",
        "def ftvectors(fname):\n",
        "  return { k:v for (k, v) in [vecref(s) for s in open(fname)] if len(v) > 1}\n",
        "\n",
        "envec = ftvectors('30k.en.vec')\n",
        "frvec = ftvectors('30k.fr.vec')\n",
        "\n",
        "# TODO: load vectors for one more language, such as zhvec (Chinese)\n",
        "# arvec = ftvectors('30k.ar.vec')\n",
        "# devec = ftvectors('30k.de.vec')\n",
        "# ptvec = ftvectors('30k.pt.vec')\n",
        "# ruvec = ftvectors('30k.ru.vec')\n",
        "esvec = ftvectors('30k.es.vec')\n",
        "# vivec = ftvectors('30k.vi.vec')\n",
        "# zhvec = ftvectors('30k.zh.vec')"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j88E1JdueZHc"
      },
      "source": [
        "**TODO**: Your next task is to write a simple function that takes a vector and a dictionary of vectors and finds the most similar item in the dictionary. For this assignment, a linear scan through the dictionary using your `cosim` function from above is acceptible."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmdirYOjoSWV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ccbd803-6ccf-4a38-ca82-27fecec30b7a"
      },
      "source": [
        "## TODO: implement this search function\n",
        "def mostSimilar(vec, vecDict):\n",
        "\n",
        "  mostSimilar = ''  # Initialize with an empty string\n",
        "  similarity = 0    # Initialize with the minimum similarity\n",
        "  for word, candidateVec in vecDict.items():\n",
        "    current_similarity = cosim(vec, candidateVec)\n",
        "    if current_similarity > similarity:\n",
        "      mostSimilar = word\n",
        "      similarity = current_similarity\n",
        "  return(mostSimilar, similarity)\n",
        "\n",
        "\n",
        "\n",
        "## some example searches\n",
        "[mostSimilar(envec[e], frvec) for e in ['computer', 'germany', 'matrix', 'physics', 'yeast']]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('informatique', 0.5023827767603765),\n",
              " ('allemagne', 0.593718413875964),\n",
              " ('matrice', 0.5088361302065517),\n",
              " ('physique', 0.4555543434796394),\n",
              " ('fermentation', 0.3504105196166514)]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIKUD5qxpUMB"
      },
      "source": [
        "Some matches make more sense than others. Note that `computer` most closely matches `informatique`, the French term for *computer science*. If you looked further down the list, you would see `ordinateur`, the term for *computer*. This is one weakness of a focus only on embeddings for word *types* independent of context.\n",
        "\n",
        "To evalute cross-language embeddings more broadly, we'll look at a dataset of links between Wikipedia articles."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "az10sIFwsEUP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84db1901-78a5-446f-db46-fd7a679f4776"
      },
      "source": [
        "!wget http://www.ccs.neu.edu/home/dasmith/courses/cs6200/links.tab\n",
        "links = [s.split() for s in open('links.tab')]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-06 21:26:06--  http://www.ccs.neu.edu/home/dasmith/courses/cs6200/links.tab\n",
            "Resolving www.ccs.neu.edu (www.ccs.neu.edu)... 52.70.229.197\n",
            "Connecting to www.ccs.neu.edu (www.ccs.neu.edu)|52.70.229.197|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://www.khoury.northeastern.edu/home/dasmith/courses/cs6200/links.tab [following]\n",
            "--2024-12-06 21:26:06--  https://www.khoury.northeastern.edu/home/dasmith/courses/cs6200/links.tab\n",
            "Resolving www.khoury.northeastern.edu (www.khoury.northeastern.edu)... 52.70.229.197\n",
            "Connecting to www.khoury.northeastern.edu (www.khoury.northeastern.edu)|52.70.229.197|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1408915 (1.3M)\n",
            "Saving to: ‘links.tab’\n",
            "\n",
            "links.tab           100%[===================>]   1.34M  3.62MB/s    in 0.4s    \n",
            "\n",
            "2024-12-06 21:26:07 (3.62 MB/s) - ‘links.tab’ saved [1408915/1408915]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqHq0hFCv8NY"
      },
      "source": [
        "This `links` variable consists of triples of `(English term, language, term in that language)`. For example, here is the link between English `academy` and French `académie`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQ7eusdxtdsq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7c86e9e-fdff-4446-c6fd-28ed02fc401b"
      },
      "source": [
        "links[302]"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['academy', 'fr', 'académie']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We construct a test set for English-French from the first 1000 links between those languages."
      ],
      "metadata": {
        "id": "oA85pbt3JL1Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "frtest = [x for x in links if x[1] == \"fr\"][0:1000]\n",
        "frtest[0:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wuYkUbkYIwSb",
        "outputId": "d531890f-15f4-408f-8c05-064ec596e831"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['aalborg', 'fr', 'aalborg'],\n",
              " ['aarhus', 'fr', 'aarhus'],\n",
              " ['aba', 'fr', 'aba'],\n",
              " ['abad', 'fr', 'abad'],\n",
              " ['abandon', 'fr', 'abandon'],\n",
              " ['abbas', 'fr', 'abbas'],\n",
              " ['abbasid', 'fr', 'abbassides'],\n",
              " ['abbess', 'fr', 'abbesse'],\n",
              " ['abbey', 'fr', 'abbaye'],\n",
              " ['abbot', 'fr', 'abbé']]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYEdOQbmwql3"
      },
      "source": [
        "**TODO**: Evaluate the English and French embeddings by computing the proportion of English Wikipedia articles whose corresponding French article in this test set `frtest` is also the closest word in embedding space. Skip English articles not covered by the word embedding dictionary.\n",
        "\n",
        "Since many articles, e.g., about named entities, have the same title in English and French, use the identity function as a baseline and compute its accuracy. In other words, how often would you find the right French articles by simply echoing the English title as if it were French? In the ten example links above, `aalborg` and `aarhus` (two cities in Denmark) are the same in English and French. Remember to iterate only over the 1000 linked Wikipedia articles in the test set, not the entire embedding dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJrTJ3ja91Z4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10937310-24ba-4df3-e92b-0caeca038635"
      },
      "source": [
        "## TODO: Compute English-French Wikipedia retrieval accuracy.\n",
        "accuracy = 0\n",
        "baselineAccuracy = 0\n",
        "\n",
        "correct_retrievals = 0\n",
        "baseline_correct = 0\n",
        "total_test_cases = 0\n",
        "\n",
        "for link in frtest:\n",
        "    eng_word = link[0]  # English term\n",
        "    fr_word = link[2]   # French term\n",
        "\n",
        "    # Skip if the English word is not in the English embedding dictionary\n",
        "    if eng_word not in envec:\n",
        "        continue\n",
        "\n",
        "    # Retrieve the most similar word in French embeddings for the English term\n",
        "    retrieved_word, _ = mostSimilar(envec[eng_word], frvec)\n",
        "\n",
        "    # Check if the retrieved word matches the actual French word\n",
        "    if retrieved_word == fr_word:\n",
        "        correct_retrievals += 1\n",
        "\n",
        "    # Baseline: Check if English and French words are identical\n",
        "    if eng_word == fr_word:\n",
        "        baseline_correct += 1\n",
        "\n",
        "    total_test_cases += 1\n",
        "\n",
        "# Compute accuracy\n",
        "if total_test_cases > 0:\n",
        "    accuracy = correct_retrievals / total_test_cases\n",
        "    baselineAccuracy = baseline_correct / total_test_cases\n",
        "\n",
        "# Print results\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Baseline Accuracy: {baselineAccuracy}\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.575\n",
            "Baseline Accuracy: 0.661\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hqd1buq-OEo"
      },
      "source": [
        "**TODO**: Compute the accuracy, i.e. precision@1, of the embeddings and of the identity function for the first 1000 links between English and another language besides French. Although the baseline will be lower for languages not written in the Roman alphabet (i.e., Arabic or Chinese), there are still many articles in those languages with headwords written in Roman characters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjnKtHya-jmj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01e7e8db-5ba6-41fb-b20f-2c3a3703a634"
      },
      "source": [
        "## TODO: Compute English-X Wikipedia retrieval accuracy.\n",
        "\n",
        "# Compute English-X Wikipedia retrieval accuracy\n",
        "def compute_accuracy(test_set, envec, xvec):\n",
        "    \"\"\"\n",
        "    Compute the accuracy (precision@1) for the embedding-based retrieval\n",
        "    and the identity function for the given test set between English and language X.\n",
        "    \"\"\"\n",
        "    correct_retrievals = 0\n",
        "    baseline_correct = 0\n",
        "    total_test_cases = 0\n",
        "\n",
        "    for link in test_set:\n",
        "        eng_word = link[0]  # English term\n",
        "        x_word = link[2]    # X language term\n",
        "\n",
        "        # Skip if the English word is not in the English embedding dictionary\n",
        "        if eng_word not in envec:\n",
        "            continue\n",
        "\n",
        "        # Retrieve the most similar word in the X embeddings for the English term\n",
        "        retrieved_word, _ = mostSimilar(envec[eng_word], xvec)\n",
        "\n",
        "        # Check if the retrieved word matches the actual X word\n",
        "        if retrieved_word == x_word:\n",
        "            correct_retrievals += 1\n",
        "\n",
        "        # Baseline: Check if English and X words are identical\n",
        "        if eng_word == x_word:\n",
        "            baseline_correct += 1\n",
        "\n",
        "        total_test_cases += 1\n",
        "\n",
        "    # Compute accuracy and baseline accuracy\n",
        "    accuracy = correct_retrievals / total_test_cases if total_test_cases > 0 else 0\n",
        "    baseline_accuracy = baseline_correct / total_test_cases if total_test_cases > 0 else 0\n",
        "\n",
        "    return accuracy, baseline_accuracy\n",
        "\n",
        "# Example usage with another language (e.g., Spanish 'esvec')\n",
        "test_set = [x for x in links if x[1] == \"es\"][:1000]  # Assuming `links` contains English-X pairs\n",
        "accuracy, baseline_accuracy = compute_accuracy(test_set, envec, esvec)  # Replace `esvec` with the target language embeddings\n",
        "\n",
        "print(f\"Accuracy for English-X retrieval: {accuracy}\")\n",
        "print(f\"Baseline accuracy: {baseline_accuracy}\")\n",
        "\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for English-X retrieval: 0.536\n",
            "Baseline accuracy: 0.518\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6z01sufFPJh"
      },
      "source": [
        "**TODO**: Find the 10 nearest neighbors of each English term to compute \"recall at 10\" and \"mean reciprocal rank at 10\"."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## TODO: Compute recall@10 and MRR@10 when retrieving 10 nearest neighbors in French and some other language.\n",
        "\n",
        "import heapq\n",
        "\n",
        "def recall_at_10_and_mrr(test_set, envec, xvec):\n",
        "    \"\"\"\n",
        "    Compute recall@10 and mean reciprocal rank (MRR)@10 for retrieving the 10 nearest neighbors\n",
        "    in the embedding space of the target language (xvec).\n",
        "    \"\"\"\n",
        "    recall_at_10 = 0\n",
        "    mrr_at_10 = 0\n",
        "    total_test_cases = 0\n",
        "\n",
        "    for link in test_set:\n",
        "        eng_word = link[0]  # English term\n",
        "        x_word = link[2]    # Ground truth word in target language\n",
        "\n",
        "        # Skip if the English word is not in the English embedding dictionary\n",
        "        if eng_word not in envec:\n",
        "            continue\n",
        "\n",
        "        # Compute similarities to all target language words\n",
        "        similarities = []\n",
        "        for target_word, target_vec in xvec.items():\n",
        "            similarity = cosim(envec[eng_word], target_vec)\n",
        "            similarities.append((similarity, target_word))\n",
        "\n",
        "        # Retrieve the top 10 most similar words\n",
        "        top_10_neighbors = heapq.nlargest(10, similarities, key=lambda x: x[0])\n",
        "\n",
        "        # Extract the words from the top 10 neighbors\n",
        "        top_10_words = [neighbor[1] for neighbor in top_10_neighbors]\n",
        "\n",
        "        # Check if the ground truth word is in the top 10 neighbors\n",
        "        if x_word in top_10_words:\n",
        "            recall_at_10 += 1\n",
        "            # Compute rank (1-based index) of the ground truth word in the top 10\n",
        "            rank = top_10_words.index(x_word) + 1\n",
        "            mrr_at_10 += 1 / rank\n",
        "\n",
        "        total_test_cases += 1\n",
        "\n",
        "    # Normalize recall@10 and MRR@10\n",
        "    recall_at_10 = recall_at_10 / total_test_cases if total_test_cases > 0 else 0\n",
        "    mrr_at_10 = mrr_at_10 / total_test_cases if total_test_cases > 0 else 0\n",
        "\n",
        "    return recall_at_10, mrr_at_10\n",
        "\n",
        "# Example usage with a target language (e.g., French 'frvec')\n",
        "test_set = [x for x in links if x[1] == \"fr\"][:1000]  # Assuming `links` contains English-French pairs\n",
        "recall_10, mrr_10 = recall_at_10_and_mrr(test_set, envec, frvec)\n",
        "\n",
        "print(f\"Recall@10: {recall_10}\")\n",
        "print(f\"MRR@10: {mrr_10}\")\n"
      ],
      "metadata": {
        "id": "TgAORWTQl0Sl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc807b90-dd04-4236-b56e-6768c85f6cb2"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recall@10: 0.644\n",
            "MRR@10: 0.6013718253968254\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Speeding up Vector Search (for CS6200)\n",
        "\n",
        "The list of Wikipedia headwords is short enough that a linear scan through the non-English language embeddings takes some time but is feasible. In a production system, you could index the word embeddings using SimHash or some other locality sensitive hashing scheme, as we discussed for duplicate detection, to speed up this process.\n",
        "\n",
        "A relatively easy way to get started with fast vector similarity search is to install Meta's `faiss` (Facebook AI Similarity Search) package and read [the tutorial](https://github.com/facebookresearch/faiss/wiki/Getting-started)."
      ],
      "metadata": {
        "id": "GsXoZaVsYMXR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Outside of colab, you may need a different package manager.\n",
        "# !apt install libomp-dev\n",
        "!pip install --upgrade faiss-cpu\n",
        "# Use this line instead if you have a GPU.\n",
        "# !python -m pip install --upgrade faiss-gpu\n",
        "import faiss\n",
        "import time\n",
        "\n",
        "def build_faiss_index(embedding_dict):\n",
        "    \"\"\"\n",
        "    Build a FAISS index for a given embedding dictionary.\n",
        "    \"\"\"\n",
        "    # Convert embedding dictionary to a numpy array and map words to indices\n",
        "    vectors = np.array(list(embedding_dict.values())).astype('float32')\n",
        "    words = list(embedding_dict.keys())\n",
        "\n",
        "    # Create FAISS index\n",
        "    index = faiss.IndexFlatL2(vectors.shape[1])  # L2 distance for nearest neighbor search\n",
        "    index.add(vectors)  # Add vectors to the index\n",
        "\n",
        "    return index, words\n",
        "\n",
        "def faiss_search(index, query_vector, top_k=10):\n",
        "    \"\"\"\n",
        "    Perform a search for the top_k nearest neighbors using a FAISS index.\n",
        "    \"\"\"\n",
        "    distances, indices = index.search(query_vector.reshape(1, -1), top_k)\n",
        "    return indices[0], distances[0]\n",
        "\n",
        "def evaluate_with_faiss(test_set, envec, xvec, index, x_words, top_k=10):\n",
        "    \"\"\"\n",
        "    Evaluate recall@10 and MRR@10 using the FAISS index.\n",
        "    \"\"\"\n",
        "    recall_at_10 = 0\n",
        "    mrr_at_10 = 0\n",
        "    total_test_cases = 0\n",
        "\n",
        "    for link in test_set:\n",
        "        eng_word = link[0]\n",
        "        x_word = link[2]\n",
        "\n",
        "        # Skip if the English word is not in the English embedding dictionary\n",
        "        if eng_word not in envec:\n",
        "            continue\n",
        "\n",
        "        # Retrieve the FAISS index for the top_k nearest neighbors\n",
        "        query_vector = np.array(envec[eng_word]).astype('float32')\n",
        "        indices, _ = faiss_search(index, query_vector, top_k)\n",
        "\n",
        "        # Map indices back to words\n",
        "        neighbors = [x_words[idx] for idx in indices]\n",
        "\n",
        "        # Compute recall@10\n",
        "        if x_word in neighbors:\n",
        "            recall_at_10 += 1\n",
        "            rank = neighbors.index(x_word) + 1\n",
        "            mrr_at_10 += 1 / rank\n",
        "\n",
        "        total_test_cases += 1\n",
        "\n",
        "    recall_at_10 = recall_at_10 / total_test_cases if total_test_cases > 0 else 0\n",
        "    mrr_at_10 = mrr_at_10 / total_test_cases if total_test_cases > 0 else 0\n",
        "    return recall_at_10, mrr_at_10\n",
        "\n",
        "# Brute-force search for comparison\n",
        "def brute_force_search(query_vector, target_dict, top_k=10):\n",
        "    \"\"\"\n",
        "    Perform a brute-force search to find the top_k nearest neighbors.\n",
        "    \"\"\"\n",
        "    similarities = []\n",
        "    for word, target_vec in target_dict.items():\n",
        "        similarity = cosim(query_vector, target_vec)\n",
        "        similarities.append((similarity, word))\n",
        "    # Sort by similarity and take the top_k\n",
        "    top_k_neighbors = sorted(similarities, key=lambda x: x[0], reverse=True)[:top_k]\n",
        "    return [neighbor[1] for neighbor in top_k_neighbors]\n",
        "\n",
        "# Evaluate Brute Force\n",
        "def evaluate_brute_force(test_set, envec, xvec, top_k=10):\n",
        "    recall_at_10 = 0\n",
        "    mrr_at_10 = 0\n",
        "    total_test_cases = 0\n",
        "\n",
        "    for link in test_set:\n",
        "        eng_word = link[0]\n",
        "        x_word = link[2]\n",
        "\n",
        "        # Skip if the English word is not in the embedding dictionary\n",
        "        if eng_word not in envec:\n",
        "            continue\n",
        "\n",
        "        # Perform brute force search\n",
        "        query_vector = np.array(envec[eng_word])\n",
        "        neighbors = brute_force_search(query_vector, xvec, top_k)\n",
        "\n",
        "        # Compute recall@10\n",
        "        if x_word in neighbors:\n",
        "            recall_at_10 += 1\n",
        "            rank = neighbors.index(x_word) + 1\n",
        "            mrr_at_10 += 1 / rank\n",
        "\n",
        "        total_test_cases += 1\n",
        "\n",
        "    recall_at_10 = recall_at_10 / total_test_cases if total_test_cases > 0 else 0\n",
        "    mrr_at_10 = mrr_at_10 / total_test_cases if total_test_cases > 0 else 0\n",
        "    return recall_at_10, mrr_at_10\n",
        "\n",
        "# Compare Brute Force and FAISS\n",
        "def compare_brute_force_faiss(test_set, envec, frvec, fr_index, fr_words):\n",
        "    print(\"Evaluating Brute Force...\")\n",
        "    start_time = time.time()\n",
        "    brute_recall_10, brute_mrr_10 = evaluate_brute_force(test_set, envec, frvec)\n",
        "    brute_time = time.time() - start_time\n",
        "    print(f\"Brute Force - Recall@10: {brute_recall_10}, MRR@10: {brute_mrr_10}, Time: {brute_time} seconds\")\n",
        "\n",
        "    print(\"\\nEvaluating FAISS...\")\n",
        "    start_time = time.time()\n",
        "    faiss_recall_10, faiss_mrr_10 = evaluate_with_faiss(test_set, envec, frvec, fr_index, fr_words)\n",
        "    faiss_time = time.time() - start_time\n",
        "    print(f\"FAISS - Recall@10: {faiss_recall_10}, MRR@10: {faiss_mrr_10}, Time: {faiss_time} seconds\")\n",
        "\n",
        "    print(\"\\nComparison:\")\n",
        "    print(f\"Recall@10 - Brute Force: {brute_recall_10}, FAISS: {faiss_recall_10}\")\n",
        "    print(f\"MRR@10 - Brute Force: {brute_mrr_10}, FAISS: {faiss_mrr_10}\")\n",
        "    print(f\"Time - Brute Force: {brute_time} seconds, FAISS: {faiss_time} seconds\")\n",
        "\n",
        "# Example usage for comparison\n",
        "compare_brute_force_faiss(frtest, envec, frvec, fr_index, fr_words)"
      ],
      "metadata": {
        "id": "KSJ-c9PDKHaD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59d546dd-217e-444b-d0fc-16d4b13995a9"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (1.9.0.post1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.2)\n",
            "Evaluating Brute Force...\n",
            "Brute Force - Recall@10: 0.644, MRR@10: 0.6013718253968254, Time: 264.80918884277344 seconds\n",
            "\n",
            "Evaluating FAISS...\n",
            "FAISS - Recall@10: 0.644, MRR@10: 0.6013718253968254, Time: 3.0005006790161133 seconds\n",
            "\n",
            "Comparison:\n",
            "Recall@10 - Brute Force: 0.644, FAISS: 0.644\n",
            "MRR@10 - Brute Force: 0.6013718253968254, FAISS: 0.6013718253968254\n",
            "Time - Brute Force: 264.80918884277344 seconds, FAISS: 3.0005006790161133 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TODO**: Create two vector indexes, for the FastText embeddings of French and for the other language you evaluated above. Use `faiss` to find the 10 nearest neighbors for the top 1000 Wikipedia headwords you evaluated for English-French and the English-X as above.\n",
        "\n",
        "First, measure the _effectiveness_ of this approximate vector search approach. How does the R@10 and MRR@10 using `faiss` compare to the brute-force search you did above?\n",
        "\n",
        "Second, measure the _efficiency_ of this approach. How long in seconds does finding nearest neighbors for 1000 headwords by brute force compare to using `faiss`? (For this exercise, don't worry about amortizing indexing costs.)"
      ],
      "metadata": {
        "id": "hfr0buVwLz-o"
      }
    }
  ]
}